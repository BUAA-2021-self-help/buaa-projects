# 《网络存储》课程设计：实践型存储设计

[TOC]

## 1. 实验目标

- [x] Ceph环境搭建与应用

注：参考CSDN，51CTO，搭建完成尝试验证存储节点的应用

[官网: Ceph](https://ceph.io/)

[官方文档：Ceph Documentation](https://docs.ceph.com/en/latest/start/intro/)

[Github官方仓库: ceph](https://github.com/ceph/ceph)

## 2. Ceph基础

### 2.1. Ceph概述

[参考：Ceph工作原理及安装](https://www.jianshu.com/p/25163032f57f)

Ceph是一个分布式存储系统，诞生于2004年，最早致力于开发下一代高性能分布式文件系统的项目。随着云计算的发展，Ceph乘上了OpenStack的春风，进而成为了开源社区受关注较高的项目之一。

- **CRUSH算法**
  - CRUSH算法是Ceph的两大创新之一，简单来说，Ceph摒弃了传统的集中式存储元数据寻址的方案，转而使用CRUSH算法完成数据的寻址操作。CRUSH在一致性哈希基础上很好的考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。CRUSH算法有相当强大的扩展性，理论上支持数千个存储节点。
- **高可用**
  - Ceph中的数据副本数量可以由管理员自行定义，并可以通过CRUSH算法指定副本的物理存储位置以分隔故障域，支持数据强一致性；Ceph可以忍受多种故障场景并自动尝试并行修复。
- **高扩展性**
  - Ceph不同于swift，客户端所有的读写操作都要经过代理节点。一旦集群并发量增大时，代理节点很容易成为单点瓶颈。Ceph本身并没有主控节点，扩展起来比较容易，并且理论上，它的性能会随着磁盘数量的增加而线性增长。
- **特性丰富**
  - Ceph支持三种调用接口：对象存储，块存储，文件系统挂载。三种方式可以一同使用。在国内一些公司的云环境中，通常会采用Ceph作为openstack的唯一后端存储来提升数据转发效率。

### 2.2. Ceph核心组件

[参考：Ceph基础知识和基础架构认识](https://www.cnblogs.com/luohaixian/p/8087591.html)

Ceph的核心组件包括Ceph OSD、Ceph Monitor和Ceph MDS。

Ceph OSD：OSD的英文全称是Object Storage Device，它的主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其它OSD间进行心跳检查等，并将一些变化情况上报给Ceph Monitor。一般情况下一块硬盘对应一个OSD，由OSD来对硬盘存储进行管理，当然一个分区也可以成为一个OSD。

Ceph OSD的架构实现由物理磁盘驱动器、Linux文件系统和Ceph OSD服务组成，对于Ceph OSD Daemon而言，Linux文件系统显性的支持了其拓展性，一般Linux文件系统有好几种，比如有BTRFS、XFS、Ext4等，BTRFS虽然有很多优点特性，但现在还没达到生产环境所需的稳定性，一般比较推荐使用XFS。

伴随OSD的还有一个概念叫做Journal盘，一般写数据到Ceph集群时，都是先将数据写入到Journal盘中，然后每隔一段时间比如5秒再将Journal盘中的数据刷新到文件系统中。一般为了使读写时延更小，Journal盘都是采用SSD，一般分配10G以上，当然分配多点那是更好，Ceph中引入Journal盘的概念是因为Journal允许Ceph OSD功能很快做小的写操作；一个随机写入首先写入在上一个连续类型的journal，然后刷新到文件系统，这给了文件系统足够的时间来合并写入磁盘，一般情况下使用SSD作为OSD的journal可以有效缓冲突发负载。

Ceph Monitor：由该英文名字我们可以知道它是一个监视器，负责监视Ceph集群，维护Ceph集群的健康状态，同时维护着Ceph集群中的各种Map图，比如OSD Map、Monitor Map、PG Map和CRUSH Map，这些Map统称为Cluster Map，Cluster Map是RADOS的关键数据结构，管理集群中的所有成员、关系、属性等信息以及数据的分发，比如当用户需要存储数据到Ceph集群时，OSD需要先通过Monitor获取最新的Map图，然后根据Map图和object id等计算出数据最终存储的位置。

Ceph MDS：全称是Ceph MetaData Server，主要保存的文件系统服务的元数据，但对象存储和块存储设备是不需要使用该服务的。

查看各种Map的信息可以通过如下命令：`ceph`、`osd`(`mon`、`pg`)、`dump`

### 2.3. Ceph基础架构组件

![fig](img/2020-12-19-23-33-37.png)

从架构图中可以看到最底层的是RADOS，RADOS自身是一个完整的分布式对象存储系统，它具有可靠、智能、分布式等特性，Ceph的高可靠、高可拓展、高性能、高自动化都是由这一层来提供的，用户数据的存储最终也都是通过这一层来进行存储的，RADOS可以说就是Ceph的核心。

RADOS系统主要由两部分组成，分别是OSD和Monitor。

基于RADOS层的上一层是LIBRADOS，LIBRADOS是一个库，它允许应用程序通过访问该库来与RADOS系统进行交互，支持多种编程语言，比如C、C++、Python等。

基于LIBRADOS层开发的又可以看到有三层，分别是RADOSGW、RBD和CEPH FS。

RADOSGW：RADOSGW是一套基于当前流行的RESTFUL协议的网关，并且兼容S3和Swift。

RBD：RBD通过Linux内核客户端和QEMU/KVM驱动来提供一个分布式的块设备。

CEPH FS：CEPH FS通过Linux内核客户端和FUSE来提供一个兼容POSIX的文件系统。

## 3. 尝试在Ubuntu 16中，通过ceph-ansible配置

> ceph-ansible
> [参考：ceph-ansible Installation](https://docs.ceph.com/projects/ceph-ansible/en/latest/)

所输的具体命令见下图中的Terminal：

![fig](img/2020-12-13-13-58-31.png)

![fig](img/2020-12-13-14-14-48.png)

![fig](img/2020-12-13-14-18-38.png)

![fig](img/2020-12-13-14-21-36.png)

Fail (未定义node-1，应该和后文中的CentOS 7中的一样，配置多台主机。)

## 4. 在Google Cloud Platform中部署Ceph

### 4.1. 部署Ceph

- 通过搜索Ceph API，一键部署成功(使用默认配置，设置了3个节点)

![fig](img/2020-12-13-18-37-54.png)

- Wait for the cluster to configure OSDs on a data node (as root)

```bash
sudo su # 进入到root
ceph status # 查看Ceph状态
```

![fig](img/2020-12-13-18-15-42.png)

可见Ceph正在运行中，状态HEALTH为OK，尚没有建立pool。

### 4.2. 创建pool

- Create a volume on one of the data nodes (as root)

```bash
ceph osd pool create vol_data 128; ceph osd pool create vol_metadata 128; ceph fs new vol vol_metadata vol_data; ceph fs ls
```

![fig](img/2020-12-13-18-19-16.png)

创建了一个osd pool

### 4.3. 挂载Ceph存储

- Allow an instance to mount Ceph volumes

(Tag an instance with ceph-1-ceph-client tag or allow traffic to TCP ports 6789-7300 on data nodes using Firewall rules.)

- Mount a volume on a Ceph client

```bash
mkdir /mnt/cephfs; mount -t ceph 10.128.0.2:6789:/ /mnt/cephfs -o name=admin,secret=AQAk59Vf2YHvBRAA/kcIaBzl9oxCwNaX1oaSeA==
```

![fig](img/2020-12-13-18-20-52.png)

成功在Cloud Shell中，通过SSH与admin节点连接并挂接了磁盘。

## 5. 在CentOS 7中配置Ceph，验证存储节点

[参考：Ceph-deploy快速部署Ceph分布式存储](https://www.cnblogs.com/kevingrace/p/9141432.html)

### 5.1. 基本环境

#### 5.1.1. 安装CentOS 7

- 在VMware中，安装4台装有CentOS 7的虚拟机(作为Ceph的4个节点)。
- 安装过程中，选择默认设置，注意需要创建用户/管理员，设置密码。如下图。

![fig](img/2020-12-13-17-32-10.png)

- 成功进入系统，并登录。

![fig](img/2020-12-13-17-35-59.png)

#### 5.1.2. 配置IP

- 试图查看主机的IP地址：

```bash
ip addr
```

![fig](img/2020-12-13-17-40-46.png)

发现在ens33没有INET这个属性，那么就没办法通过IP远程连接。

- 设置配置文件

```bash
vi /etc/sysconfig/network-scripts/ifcfg-ens33
```

![fig](img/2020-12-13-17-47-18.png)

从配置文件中可以看出CentOS 7默认是不启动网卡的(ONBOOT=no)。我们把这一项修改为yes。

![fig](img/2020-12-13-17-48-26.png)

试图通过`ESC`进入到命令模式后，通过`:x!`保存

![fig](img/2020-12-13-17-49-19.png)

然而发现是可读文件，在当前权限下，无法修改。

通过以下指令修改权限：

```bash
chmod 777 /etc/sysconfig/network-scripts/ifcfg-ens33 # 777是最高权限
```

![fig](img/2020-12-13-17-53-32.png)

(普通用户无法设置777权限，进入到root用户后修改权限成功)

- 再次从vi进入到配置文件，此次修改配置文件成功。

![fig](img/2020-12-13-17-54-50.png)

- 重启网络服务：

```bash
sudo service network restart
```

![fig](img/2020-12-13-17-56-37.png)

- 再次查看IP地址：

![fig](img/2020-12-13-17-58-09.png)

可见该机的IP为：192.168.61.156

- 按以上步骤重复，最终4台主机所设置的用户(管理员)名及主机的IP地址的对应关系可得：

![fig](img/2020-12-13-18-27-30.png)

ceph-node1: 192.168.61.157

![fig](img/2020-12-13-19-23-18.png)

ceph-node2: 192.168.61.158

![fig](img/2020-12-13-19-24-58.png)

ceph-node3: 192.168.61.159

| IP             | Hostname   | 角色|
| -------------- | ---------- | --------|
| 192.168.61.156 | ceph-admin | mds1、mon1 |
| 192.168.61.157 | ceph-node1 | osd1 |
| 192.168.61.158 | ceph-node2 | osd2 |
| 192.168.61.159 | ceph-node3 | osd3 |

> osd: ceph-osd is the object storage daemon for the Ceph distributed file system. It is responsible for storing objects on a local file system and providing access to them over the network.

（第二天接着做该实验，各节点的IP变成了：）

| IP             | Hostname   |
| -------------- | ---------- |
| 192.168.61.160 | ceph-admin |
| 192.168.61.162 | ceph-node1 |
| 192.168.61.163 | ceph-node2 |
| 192.168.61.161 | ceph-node3 |

#### 5.1.3. 配置主机名映射

- 为每个节点修改主机名：

```bash
# 在ceph-admin主机中
hostnamectl set-hostname ceph-admin
```

![fig](img/2020-12-13-19-31-21.png)

```bash
# 类似地，在ceph-node1主机中：
hostnamectl set-hostname ceph-node1
# 在ceph-node2主机中：
hostnamectl set-hostname ceph-node2
# 在ceph-node3主机中：
hostnamectl set-hostname ceph-node3
```

- 为每个节点绑定主机名映射

在每一台主机中：

```bash
vi /etc/hosts
# 添加以下条目：
192.168.61.156 ceph-admin # 第二天接着做该实验变成了：192.168.61.160
192.168.61.157 ceph-node1 # 第二天接着做该实验变成了：192.168.61.162
192.168.61.158 ceph-node2 # 第二天接着做该实验变成了：192.168.61.163
192.168.61.159 ceph-node3 # 第二天接着做该实验变成了：192.168.61.161
```

![fig](img/2020-12-13-19-41-09.png)

- 每个节点确认连通性

```bash
ping -c 3 ceph-admin
ping -c 3 ceph-node1
ping -c 3 ceph-node2
ping -c 3 ceph-node3
```

![fig](img/2020-12-13-19-47-56.png)

如上图所示，在ceph-admin中连接其他node(包括自己)，全部连通。其余主机之间，亦能互相连通。

#### 5.1.4. 关闭防火墙和SELinux

- 每个节点关闭防火墙和SELinux

```bash
systemctl stop firewalld
systemctl disable firewalld
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
setenforce 0
```

![fig](img/2020-12-18-22-40-01.png)

#### 5.1.5. 配置NTP

- 每个节点安装和配置NTP（官方推荐的是集群的所有节点全部安装并配置NTP，需要保证各节点的系统时间一致。没有自己部署NTP服务器，就在线同步NTP）

```bash
yum install ntp ntpdate ntp-doc -y
systemctl restart ntpd
systemctl status ntpd
```

![fig](img/2020-12-18-22-44-13.png)

![fig](img/2020-12-18-22-45-00.png)

#### 5.1.6. 配置国内镜像源

- 每个节点准备yum源
- 删除默认的源（国外的比较慢）

```bash
yum clean all
mkdir /mnt/bak
mv /etc/yum.repos.d/* /mnt/bak/
```

![fig](img/2020-12-18-22-46-29.png)

- 下载阿里云的base源和epel源

```bash
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
```

- 添加ceph源

```bash
# vi /etc/yum.repos.d/ceph.repo
[ceph]
name=ceph
baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/
gpgcheck=0
priority =1
[ceph-noarch]
name=cephnoarch
baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/
gpgcheck=0
priority =1
[ceph-source]
name=Ceph source packages
baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/SRPMS
gpgcheck=0
priority=1
```

![fig](img/2020-12-18-23-05-18.png)

#### 5.1.7. 创建Ceph用户

- 为每个节点创建cephuser用户，设置sudo权限

```bash
useradd -d /home/cephuser -m cephuser
echo "cephuser"|passwd --stdin cephuser # 用户名和密码都是cephuser
echo "cephuser ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephuser
chmod 0440 /etc/sudoers.d/cephuser
sed -i s'/Defaults requiretty/#Defaults requiretty'/g /etc/sudoers
```

![fig](img/2020-12-18-23-14-04.png)

- 测试cephuser的sudo权限

```
# su - cephuser
$ sudo su -
```

![fig](img/2020-12-18-23-17-26.png)

#### 5.1.8. 配置SSH

- 配置相互间的SSH信任关系
- 先在ceph-admin节点上产生公私钥文件，然后将ceph-admin节点的.ssh目录拷贝给其他节点

```
[root@ceph-admin ~]# su - cephuser
[cephuser@ceph-admin ~]$ ssh-keygen -t rsa    #一路回车
[cephuser@ceph-admin ~]$ cd .ssh/
[cephuser@ceph-admin .ssh]$ ls
id_rsa  id_rsa.pub
[cephuser@ceph-admin .ssh]$ cp id_rsa.pub authorized_keys

[cephuser@ceph-admin .ssh]$ scp -r /home/cephuser/.ssh ceph-node1:/home/cephuser/
[cephuser@ceph-admin .ssh]$ scp -r /home/cephuser/.ssh ceph-node2:/home/cephuser/
[cephuser@ceph-admin .ssh]$ scp -r /home/cephuser/.ssh ceph-node3:/home/cephuser/
```

![fig](img/2020-12-18-23-37-35.png)

- 然后在各节点之间验证cephuser用户下的SSH相互信任关系

```
$ ssh -p22 cephuser@ceph-admin
$ ssh -p22 cephuser@ceph-node1
$ ssh -p22 cephuser@ceph-node2
$ ssh -p22 cephuser@ceph-node3
```

![fig](img/2020-12-18-23-40-18.png)

### 5.2. 准备磁盘

> ceph-node1、ceph-node2、ceph-node3三个节点

#### 5.2.1. 新建磁盘

测试时使用的磁盘不要太小，否则后面添加磁盘时会报错，建议磁盘大小为20G及以上。

- `fdisk -l`查看当前各节点的磁盘情况（4个节点的配置一样）：

![fig](img/2020-12-19-00-07-19.png)

有约30G容量的一块磁盘（sda）。

- 在VMware为每个节点新建一块磁盘，容量为20G

![fig](img/2020-12-19-00-13-01.png)

创建磁盘，选择不split成多块

![fig](img/2020-12-19-00-13-09.png)

ceph-node1新建磁盘后的配置详情（其余类似）

![fig](img/2020-12-19-00-15-51.png)

重启后，可见多出了一块大小为20G的磁盘sdb

#### 5.2.2. 创建分区

- 为该磁盘sdb创建分区 (应该在这里也可以不做，因为后续还要继续格式化分区等。我仅在ceph-admin做了创建分区的操作)

```bash
# fdisk /dev/sdb 
# 依次输入n, p, 1, w
# 其中n分别表示创建一个新分区，p表示分区类型为主分区，1表示分区编号是1，w表示保存
```

![fig](img/2020-12-19-00-24-54.png)

- 检查磁盘

```bash
# fdisk -l /dev/sdb
```

![fig](img/2020-12-19-00-30-08.png)

#### 5.2.3. 格式化磁盘

- 格式化磁盘

```bash
# parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100%
# mkfs.xfs /dev/sdb -f
```

![fig](img/2020-12-19-00-32-36.png)

- 查看磁盘格式（xfs格式）

```bash
# blkid -o value -s TYPE /dev/sdb
```

![fig](img/2020-12-19-00-33-18.png)

### 5.3. 部署阶段

> ceph-admin节点上使用ceph-deploy快速部署

#### 5.3.1. 安装ceph-deploy

- 进入到cephuser用户

```
[root@ceph-admin ~]# su - cephuser
```

![fig](img/2020-12-19-00-37-16.png)

- 安装ceph-deploy

```
[cephuser@ceph-admin ~]$ sudo yum update -y && sudo yum install ceph-deploy -y
```

![fig](img/2020-12-19-00-41-06.png)

安装完成。

#### 5.3.2. 创建集群

- 创建cluster目录

```
[cephuser@ceph-admin ~]$ mkdir cluster
[cephuser@ceph-admin ~]$ cd cluster/
```

![fig](img/2020-12-19-00-45-51.png)

- 创建集群（后面填写Monitor节点的主机名，这里Monitor节点和管理节点是同一台机器，即ceph-admin）

```
[cephuser@ceph-admin cluster]$ ceph-deploy new ceph-admin
```

![fig](img/2020-12-19-00-48-55.png)

- 修改ceph.conf文件
  - （注意：mon_host必须和public network 网络是同网段内！）

```bash
[cephuser@ceph-admin cluster]$ vi ceph.conf # 添加下面两行配置内容
......
public network = 192.168.61.160/24
osd pool default size = 3
```

![fig](img/2020-12-19-00-52-22.png)

#### 5.3.3. 安装Ceph

- 安装Ceph

```bash
[cephuser@ceph-admin cluster]$ ceph-deploy install ceph-admin ceph-node1 ceph-node2 ceph-node3
```

![fig](img/2020-12-19-00-58-53.png)

各节点上都成功装上了Ceph。

#### 5.3.4. 配置监控节点

- 初始化Monitor监控节点

```bash
[cephuser@ceph-admin cluster]$ ceph-deploy gatherkeys ceph-admin
```

![fig](img/2020-12-19-01-01-04.png)

- 收集所有密钥

```bash
[cephuser@ceph-admin cluster]$ ceph-deploy gatherkeys ceph-admin
```

![fig](img/2020-12-19-01-01-39.png)

#### 5.3.5. 添加OSD到集群

- 检查OSD节点上所有可用的磁盘

```bash
[cephuser@ceph-admin cluster]$ ceph-deploy disk list ceph-node1 ceph-node2 ceph-node3
```

![fig](img/2020-12-19-01-02-36.png)

- 使用zap选项删除所有osd节点上的分区

```bash
[cephuser@ceph-admin cluster]$ ceph-deploy disk zap ceph-node1:/dev/sdb ceph-node2:/dev/sdb ceph-node3:/dev/sdb
```

![fig](img/2020-12-19-01-03-52.png)

- 准备OSD（使用prepare命令）

```bash
[cephuser@ceph-admin cluster]$ ceph-deploy osd prepare ceph-node1:/dev/sdb ceph-node2:/dev/sdb ceph-node3:/dev/sdb
```

![fig](img/2020-12-19-01-06-09.png)

- 激活OSD（注意由于Ceph对磁盘进行了分区，/dev/sdb磁盘分区为/dev/sdb1）

```bash
[cephuser@ceph-admin cluster]$ ceph-deploy osd activate ceph-node1:/dev/sdb1 ceph-node2:/dev/sdb1 ceph-node3:/dev/sdb1
```

![fig](img/2020-12-19-01-07-32.png)

- 在三个osd节点上通过命令已显示磁盘已成功mount：

```
[root@ceph-node1 ceph-node1]# lsblk
```

![fig](img/2020-12-19-01-08-42.png)

在ceph-node1节点的虚拟机中输入如上命令，可见sdb-sdb1中已有ceph-0，表明Ceph挂载成功。

![fig](img/2020-12-19-01-11-33.png)

从ceph-admin通过SSH进入到ceph-node2，同样可见sdb-sdb1中已有ceph-0，表明Ceph挂载成功。（ceph-node3类似）

- 查看OSD

```bash
[cephuser@ceph-admin cluster]$ ceph-deploy disk list ceph-node1 ceph-node2 ceph-node3
# cd到cluster目录下
```

![fig](img/2020-12-19-10-00-00.png)

node1、node2、node3均有如上的两个分区(sdb1、sdb2)，表明成功了。

#### 5.3.6. 简化Ceph命令行

- 用ceph-deploy把配置文件和admin密钥拷贝到管理节点和Ceph节点，这样每次执行Ceph命令行时就无需指定Monitor节点地址和ceph.client.admin.keyring了

```bash
[cephuser@ceph-admin cluster]$ ceph-deploy admin ceph-admin ceph-node1 ceph-node2 ceph-node3
```

![fig](img/2020-12-19-10-02-20.png)

- 修改密钥权限

```bash
[cephuser@ceph-admin cluster]$ sudo chmod 644 /etc/ceph/ceph.client.admin.keyring
```

![fig](img/2020-12-19-10-03-02.png)

#### 5.3.7. 检查Ceph、OSD等状态

- 检查Ceph状态

```bash
[cephuser@ceph-admin cluster]$ sudo ceph health
HEALTH_OK
[cephuser@ceph-admin cluster]$ sudo ceph -s
```

![fig](img/2020-12-19-10-03-59.png)

- 查看ceph osd运行状态

```bash
[cephuser@ceph-admin ~]$ ceph osd stat
```

![fig](img/2020-12-19-10-04-44.png)

- 查看osd的目录树

```bash
[cephuser@ceph-admin ~]$ ceph osd tree
```

![fig](img/2020-12-19-10-05-33.png)

- 查看Monitor监控节点的服务情况

```bash
[cephuser@ceph-admin cluster]$ sudo systemctl status ceph-mon@ceph-admin
```

![fig](img/2020-12-19-10-06-39.png)

```bash
[cephuser@ceph-admin cluster]$ ps -ef|grep ceph|grep 'cluster'
```

![fig](img/2020-12-19-10-07-20.png)

- 分别查看下ceph-node1、ceph-node2、ceph-node3三个节点的osd服务情况，发现已经在启动中。

ceph-node1:

```bash
ssh -p22 cephuser@ceph-node1 # 先进入到ceph-node1
[cephuser@ceph-node1 ~]$ sudo systemctl status ceph-osd@0.service
# 启动是start、重启是restart
[cephuser@ceph-node1 ~]$ sudo ps -ef|grep ceph|grep "cluster"
```

![fig](img/2020-12-19-10-10-45.png)

loaded，服务在启动中

ceph-node2:

```bash
ssh -p22 cephuser@ceph-node2 # 先进入到ceph-node2
[cephuser@ceph-node1 ~]$ sudo systemctl status ceph-osd@0.service
# 启动是start、重启是restart
[cephuser@ceph-node1 ~]$ sudo ps -ef|grep ceph|grep "cluster"
```

![fig](img/2020-12-19-10-12-39.png)

loaded，服务在启动中

ceph-node3:

```bash
ssh -p22 cephuser@ceph-node3 # 先进入到ceph-node3
[cephuser@ceph-node1 ~]$ sudo systemctl status ceph-osd@0.service
# 启动是start、重启是restart
[cephuser@ceph-node1 ~]$ sudo ps -ef|grep ceph|grep "cluster"
```

![fig](img/2020-12-19-10-13-26.png)

loaded，服务在启动中

### 5.4. 创建文件系统

#### 5.4.1. 查看并创建管理节点状态

- 先查看管理节点状态，默认是没有管理节点的。

```bash
[cephuser@ceph-admin ~]$ ceph mds stat
e1:
```

![fig](img/2020-12-19-10-18-42.png)

- 创建管理节点（ceph-admin作为管理节点）。

需要注意：如果不创建MDS管理节点，Client客户端将不能正常挂载到Ceph集群！！

```bash
[cephuser@ceph-admin ~]$ pwd
/home/cephuser
[cephuser@ceph-admin ~]$ cd cluster/
[cephuser@ceph-admin cluster]$ ceph-deploy mds create ceph-admin
```

![fig](img/2020-12-19-10-26-39.png)

- 再次查看管理节点状态，发现已经在启动中

```bash
[cephuser@ceph-admin cluster]$ ceph mds stat
e2:, 1 up:standby
```

![fig](img/2020-12-19-10-28-50.png)

```bash
[cephuser@ceph-admin cluster]$ sudo systemctl status ceph-mds@ceph-admin
[cephuser@ceph-admin cluster]$ ps -ef|grep cluster|grep ceph-mds
```

![fig](img/2020-12-19-10-29-45.png)

#### 5.4.2. 创建Pool

- 创建Pool。Pool是Ceph存储数据时的逻辑分区，它起到namespace的作用

```bash
[cephuser@ceph-admin cluster]$ ceph osd lspools # 先查看pool
0 rbd,
```

![fig](img/2020-12-19-10-30-45.png)

- 新创建的Ceph集群只有rdb一个pool。这时需要创建一个新的pool

```bash
[cephuser@ceph-admin cluster]$ ceph osd pool create cephfs_data 10
# 后面的数字是PG的数量
pool 'cephfs_data' created
```

![fig](img/2020-12-19-10-32-15.png)

```bash
[cephuser@ceph-admin cluster]$ ceph osd pool create cephfs_metadata 10
# 创建pool的元数据
pool 'cephfs_metadata' created
```

![fig](img/2020-12-19-10-34-26.png)

```bash
[cephuser@ceph-admin cluster]$ ceph fs new myceph cephfs_metadata cephfs_data
new fs with metadata pool 2 and data pool 1
```

![fig](img/2020-12-19-10-39-42.png)

- 再次查看pool状态

```bash
[cephuser@ceph-admin cluster]$ ceph osd lspools
0 rbd,1 cephfs_data,2 cephfs_metadata,
```

#### 5.4.3. 检查MDS、Ceph集群状态

![fig](img/2020-12-19-10-40-35.png)

- 检查MDS管理节点状态

```bash
[cephuser@ceph-admin cluster]$ ceph mds stat
e5: 1/1/1 up {0=ceph-admin=up:active}
```
![fig](img/2020-12-19-10-41-20.png)

- 查看Ceph集群状态

```bash
[cephuser@ceph-admin cluster]$ sudo ceph -s
```

![fig](img/2020-12-19-10-42-00.png)

3 osds: 3 up, 3 in。均正常运行中。

- 查看Ceph集群端口

```bash
# 在root状态下安装lsof(用于查看文件的打开情况，用于调试程序，查看系统情况)
# yum -y install lsof
[root@ceph-admin cluster]# sudo lsof -i:6789
```

![fig](img/2020-12-19-10-47-53.png)

端口全部established/listen

### 5.5. Client端挂载Ceph存储（采用fuse方式）

#### 5.5.1. 配置Client节点

- 创建ceph-client虚拟机（CentOS 7系统），继续实验

![fig](img/2020-12-19-11-00-53.png)

- 按先前方法配置网络（启动网卡）

![fig](img/2020-12-19-11-04-26.png)

可以ping通先前创的各个Ceph节点了。

#### 5.5.2. 安装ceph-fuse

- 安装ceph-fuse

```bash
rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
# EPEL（Extra Packages for Enterprise Linux） 是由 Fedora 社区打造，为 RHEL 及衍生发行版如 CentOS等提供高质量软件包的项目。装上了EPEL，就像在 Fedora 上一样，可以通过 yum install 软件包名，即可安装很多以前需要编译安装的软件、常用的软件或一些比较流行的软件，比如现在流行的nginx, openvpn等等，都可以使用EPEL很方便的安装更新。
yum install -y ceph-fuse
```

- 如果安装失败，先执行以下命令，再执行上述安装ceph-fuse的命令

```bash
yum -y install epel-release
rpm -Uhv http://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-1.el7.noarch.rpm
```

![fig](img/2020-12-19-11-12-47.png)

安装成功。

#### 5.5.3. 创建挂载目录

- 创建挂载目录

```
# mkdir /cephfs
```

![fig](img/2020-12-19-11-13-38.png)

#### 5.5.4. 配置rsync

- 安装rsync（给ceph-admin节点也要装）

```bash
yum -y install rsync  
#启动rsync服务
systemctl start rsyncd.service
systemctl enable rsyncd.service
```

![fig](img/2020-12-19-11-19-22.png)

```bash
# 检查是否已经成功启动
# yum -y install net-tools (可以生成ifconfig命令, netstat命令)
netstat -lnp|grep 873
```

![fig](img/2020-12-19-11-21-21.png)

服务已启动。

#### 5.5.5. 复制配置和密钥

- 复制配置文件

将ceph配置文件ceph.conf从管理节点copy到client节点（192.168.10.220为管理节点）

```bash
[root@localhost ceph-client]# rsync -e "ssh -p22" -avpgolr root@192.168.61.160:/etc/ceph/ceph.conf /etc/ceph/
```

或者（两个路径下的文件内容一样）

```bash
# rsync -e "ssh -p22" -avpgolr root@192.168.61.160:/home/cephuser/cluster/ceph.conf /etc/ceph/
```

![fig](img/2020-12-19-11-24-46.png)

成功复制。

- 复制密钥

将ceph的ceph.client.admin.keyring从管理节点copy到client节点

```bash
[root@localhost ceph-client]# rsync -e "ssh -p22" -avpgolr root@192.168.61.160:/etc/ceph/ceph.client.admin.keyring /etc/ceph/
```

或者

```bash
[root@localhost ceph-client]# rsync -e "ssh -p22" -avpgolr root@192.168.61.160:/home/cephuser/cluster/ceph.client.admin.keyring /etc/ceph/
```

![fig](img/2020-12-19-11-28-05.png)

成功复制。

#### 5.5.6. 查看Ceph授权，挂载Ceph集群至Client端

- 从admin节点，查看Ceph授权

```bash
[root@ceph-admin cluster]# ceph auth list
```

![fig](img/2020-12-19-11-33-30.png)

- 将ceph集群存储挂载到客户机的/cephfs目录下

```bash
[root@localhost ceph-client]# ceph-fuse -m 192.168.61.160:6789 /cephfs
```

![fig](img/2020-12-19-12-08-28.png)

- 查看挂载情况

```bash
[root@localhost ceph-client]# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/centos-root
                       17G  1.4G   16G   8% /
tmpfs                 487M     0  487M   0% /dev/shm
...
ceph-fuse              45G  324M   45G   1% /cephfs
```

![fig](img/2020-12-19-12-12-24.png)

由上可知，已经成功挂载了Ceph存储。其中三个OSD节点，每个节点各15G。挂载成功后，便可至挂载后的目录（/cephfs）中进行文件读写了。

（在节点上通过`lsblk`命令可以查看Ceph Data分区大小）如：

![fig](img/2020-12-19-12-13-42.png)

- 取消Ceph存储的挂载的方法：

[root@localhost ceph-client]# umount /cephfs

![fig](img/2020-12-19-12-14-33.png)

### 5.6. Client端测试OSD节点发生意外

#### 5.6.1. 数据准备

- 进入挂载目录`/cephfs`

![fig](img/2020-12-19-23-37-40.png)

- 创建文件夹f、文件b.txt

![fig](img/2020-12-19-23-40-07.png)

- 在b.txt中写入一些文字

![fig](img/2020-12-19-23-41-19.png)

#### 5.6.2. 模拟挂掉1个主机

- 将ceph-node1节点挂起，模拟挂掉1个主机

![fig](img/2020-12-19-23-44-42.png)

- 通过vi打开b.txt，查看内容

![fig](img/2020-12-19-23-45-18.png)

正常显示。Ceph存储使用仍然正常。

#### 5.6.3. 模拟挂掉2个主机

- 将ceph-node2节点也挂起，模拟挂掉2个主机（一半以上的主机宕掉）

![fig](img/2020-12-19-23-47-34.png)

当前各节点状态

- 试图通过`ls`查看当前挂载目录下的文件

![fig](img/2020-12-19-23-46-39.png)

此时一直卡在该状态（_不停闪烁），无法观察到先前的文件存在，表明Ceph存储不能使用了

#### 5.6.4. 模拟1个主机恢复

- 重新将ceph-node2节点打开，查看目录中的文件。

![fig](img/2020-12-19-23-49-05.png)

重新恢复正常。

#### 5.6.5. 小结

当有一半以上的OSD节点挂掉后，远程客户端挂载的Ceph存储就会使用异常了，即暂停使用。比如本实验中有3个OSD节点，当其中一个OSD节点挂掉后（比如宕机），客户端挂载的Ceph存储使用正常；但当有2个OSD节点挂掉后，客户端挂载的Ceph存储就不能正常使用了（表现为Ceph存储目录下的数据读写操作一直卡着的状态），当OSD节点恢复后，Ceph存储也会恢复正常使用。OSD节点宕机重新启动后，OSD程序会自动运行起来（通过监控节点）

## 6. 实验总结与感想

本次实践型存储设计的大作业需要我们安装配置Ceph，并验证存储节点。我首先创建了4台CentOS 7主机，一台作为管理节点兼Monitor节点，另外三台作为分布式OSD节点。通过创建集群、在第5台CentOS 7主机中连接Ceph集群，挂载存储，模拟节点宕机后查看存储情况等方式，我验证了Ceph存储节点的各种应用。从中，我学会了非常多的Linux操作系统中的操作，如配置网卡；使用SSH连接其他主机；配置NTP；创建磁盘、管理磁盘；管理防火墙；配置镜像国内源……更初步掌握了Ceph系统的许多命令行操作，如查看各节点状态；创建集群；创建pool；挂载Ceph存储等。

这个大作业确实非常非常难——实验中要为5台近乎于全裸环境，还不能连上网的主机从配置网卡开始做起，全程黑框框命令行操作，到创建起能彼此互连的Ceph分布式存储系统。过程中，我经历了找不到靠谱的网络教程想要放弃，连不上网的无奈，命令行操作不能复制粘贴代码全部手敲，还要重复5遍的苦涩，遇到莫名其妙而BUG抓耳挠腮，尝试多种方式、多种工具（Google Cloud Platform，ceph-ansible，ceph-deploy，ceph-fuse……）……这种种困难。好在我坚持了下来，最终顺利地完成了实验，成功地为每一个节点装上了Ceph，创建了Ceph集群，并在Client端挂载Ceph存储达到安全分布式存储。

这个实验很难，但正因为它难，我学会了在看不到尽头时，黑暗中执着寻找答案，不放弃，选择坚持。事实证明，这是成功的必要条件。我做到了。且不论实验中我学会了多少技术上的知识，仅凭这一份坚持，本实验给我带来的收获便已超越了一切。

**声明：**

所有的参考资料均已在文中给出来源。实验步骤由于实验环境（如IP地址、CentOS版本、虚拟机安装方式、磁盘空间大小等）的不同，与参考资料有很多差异。图全部由我本人所截，如有需要证明，请联系我。