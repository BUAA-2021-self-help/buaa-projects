# 《网络存储》课程设计：实践型存储设计

## 实验目标

- [ ] Ceph环境搭建与应用

注：参考CSDN，51CTO，搭建完成尝试验证存储节点的应用

## Ceph概述

[参考：Ceph工作原理及安装](https://www.jianshu.com/p/25163032f57f)

Ceph是一个分布式存储系统，诞生于2004年，最早致力于开发下一代高性能分布式文件系统的项目。随着云计算的发展，Ceph乘上了OpenStack的春风，进而成为了开源社区受关注较高的项目之一。

### CRUSH算法

CRUSH算法是Ceph的两大创新之一，简单来说，Ceph摒弃了传统的集中式存储元数据寻址的方案，转而使用CRUSH算法完成数据的寻址操作。CRUSH在一致性哈希基础上很好的考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。CRUSH算法有相当强大的扩展性，理论上支持数千个存储节点。

### 高可用

Ceph中的数据副本数量可以由管理员自行定义，并可以通过CRUSH算法指定副本的物理存储位置以分隔故障域，支持数据强一致性；Ceph可以忍受多种故障场景并自动尝试并行修复。

### 高扩展性

Ceph不同于swift，客户端所有的读写操作都要经过代理节点。一旦集群并发量增大时，代理节点很容易成为单点瓶颈。Ceph本身并没有主控节点，扩展起来比较容易，并且理论上，它的性能会随着磁盘数量的增加而线性增长。

### 特性丰富

Ceph支持三种调用接口：对象存储，块存储，文件系统挂载。三种方式可以一同使用。在国内一些公司的云环境中，通常会采用Ceph作为openstack的唯一后端存储来提升数据转发效率。